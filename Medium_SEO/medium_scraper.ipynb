{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.expected_conditions import visibility_of_element_located\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.46M/6.46M [00:00<00:00, 10.1MB/s]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5604\\3494290274.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "website = \"medium\"\n",
    "\n",
    "start_date = now.strftime('%m%d%Y')\n",
    "# make data/playlist/ directory\n",
    "os.makedirs(website + '/' + start_date , exist_ok=True)\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "\n",
    "url = r'http://medium.com/'\n",
    "\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(8)\n",
    "# Log in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles found: 245\n"
     ]
    }
   ],
   "source": [
    "driver.maximize_window()\n",
    "keyword = 'Data Science'\n",
    "\n",
    "# best_of (options):  This week , This month , This year , 'All time\n",
    "\n",
    "best_of  = 'This month'\n",
    "\n",
    "os.makedirs(website + '/' + start_date + '/' + best_of, exist_ok=True)\n",
    "\n",
    "search_field = driver.find_element(By.XPATH, '//input[@placeholder=\"Search\"]')\n",
    "search_field.clear()\n",
    "search_field.send_keys(keyword)\n",
    "search_field.send_keys(Keys.ENTER)\n",
    "\n",
    "keyword_tag = WebDriverWait(driver, 20).until(\n",
    "                    EC.element_to_be_clickable(driver.find_element(By.XPATH, f\"//div[normalize-space()='{keyword}']\")))\n",
    "keyword_tag.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "best_a = WebDriverWait(driver, 20).until(\n",
    "                    EC.element_to_be_clickable(driver.find_element(By.XPATH, f\"//a[normalize-space()='Best']\")))\n",
    "best_a.click()\n",
    "drop_down = WebDriverWait(driver, 20).until(\n",
    "                    EC.element_to_be_clickable(driver.find_element(By.XPATH, \"//span[normalize-space()='This year']\")))\n",
    "drop_down.click()\n",
    "all_time = WebDriverWait(driver, 20).until(\n",
    "                    EC.element_to_be_clickable(driver.find_element(By.XPATH, f\"//button[normalize-space()='{best_of}']\")))\n",
    "all_time.click()\n",
    "\n",
    "allowance = 40\n",
    "target = 200\n",
    "scroll_speed = 20\n",
    "\n",
    "target_articles = target + allowance\n",
    "scroll_count = 0\n",
    "\n",
    "while True:\n",
    "    scroll_count += scroll_speed\n",
    "    driver.execute_script('window.scrollTo(0, {});'.format(scroll_count))\n",
    "\n",
    "    article_count =  int(driver.execute_script(\"return document.getElementsByTagName('article').length\"))\n",
    "\n",
    "    if article_count >= target_articles:\n",
    "        print(f\"number of articles found: {article_count}\")\n",
    "        break\n",
    "\n",
    "# Get links within articles\n",
    "article_urls = []\n",
    "\n",
    "articles = driver.find_elements(By.TAG_NAME, \"article\")\n",
    "\n",
    "for article in articles:\n",
    "    article_url = article.find_element(By.TAG_NAME, \"h2\").find_element(By.XPATH, \"..\").find_element(By.XPATH, \"..\").get_attribute('href')\n",
    "    article_urls.append(article_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start! link: 1, page: 1, adjustment:0\n",
      "Data saved for link: 1, page: 1\n",
      "Start! link: 2, page: 2, adjustment:0\n",
      "Data saved for link: 2, page: 2\n",
      "Start! link: 3, page: 3, adjustment:0\n",
      "Data saved for link: 3, page: 3\n",
      "Start! link: 4, page: 4, adjustment:0\n",
      "Data saved for link: 4, page: 4\n",
      "Start! link: 5, page: 5, adjustment:0\n",
      "Data saved for link: 5, page: 5\n",
      "Start! link: 6, page: 6, adjustment:0\n",
      "Data saved for link: 6, page: 6\n",
      "Start! link: 7, page: 7, adjustment:0\n",
      "Data saved for link: 7, page: 7\n",
      "Start! link: 8, page: 8, adjustment:0\n",
      "Data saved for link: 8, page: 8\n",
      "Start! link: 9, page: 9, adjustment:0\n",
      "Data saved for link: 9, page: 9\n",
      "Start! link: 10, page: 10, adjustment:0\n",
      "Data saved for link: 10, page: 10\n",
      "Start! link: 11, page: 11, adjustment:0\n",
      "Data saved for link: 11, page: 11\n",
      "Start! link: 12, page: 12, adjustment:0\n",
      "Data saved for link: 12, page: 12\n",
      "Start! link: 13, page: 13, adjustment:0\n",
      "Data saved for link: 13, page: 13\n",
      "Start! link: 14, page: 14, adjustment:0\n",
      "Data saved for link: 14, page: 14\n",
      "Start! link: 15, page: 15, adjustment:0\n",
      "Data saved for link: 15, page: 15\n",
      "Start! link: 16, page: 16, adjustment:0\n",
      "Data saved for link: 16, page: 16\n",
      "Start! link: 17, page: 17, adjustment:0\n",
      "Data saved for link: 17, page: 17\n",
      "Start! link: 18, page: 18, adjustment:0\n",
      "Data saved for link: 18, page: 18\n",
      "Start! link: 19, page: 19, adjustment:0\n",
      "Data saved for link: 19, page: 19\n",
      "Start! link: 20, page: 20, adjustment:0\n",
      "Data saved for link: 20, page: 20\n",
      "Start! link: 21, page: 21, adjustment:0\n",
      "Data saved for link: 21, page: 21\n",
      "Start! link: 22, page: 22, adjustment:0\n",
      "Data saved for link: 22, page: 22\n",
      "Start! link: 23, page: 23, adjustment:0\n",
      "Data saved for link: 23, page: 23\n",
      "Start! link: 24, page: 24, adjustment:0\n",
      "Data saved for link: 24, page: 24\n",
      "Start! link: 25, page: 25, adjustment:0\n",
      "Data saved for link: 25, page: 25\n",
      "Start! link: 26, page: 26, adjustment:0\n",
      "Data saved for link: 26, page: 26\n",
      "Start! link: 27, page: 27, adjustment:0\n",
      "Data saved for link: 27, page: 27\n",
      "Start! link: 28, page: 28, adjustment:0\n",
      "Data saved for link: 28, page: 28\n",
      "Start! link: 29, page: 29, adjustment:0\n",
      "Data saved for link: 29, page: 29\n",
      "Start! link: 30, page: 30, adjustment:0\n",
      "Data saved for link: 30, page: 30\n",
      "Start! link: 31, page: 31, adjustment:0\n",
      "Data saved for link: 31, page: 31\n",
      "Start! link: 32, page: 32, adjustment:0\n",
      "Data saved for link: 32, page: 32\n",
      "Start! link: 33, page: 33, adjustment:0\n",
      "Data saved for link: 33, page: 33\n",
      "Start! link: 34, page: 34, adjustment:0\n",
      "Data saved for link: 34, page: 34\n",
      "Start! link: 35, page: 35, adjustment:0\n",
      "Data saved for link: 35, page: 35\n",
      "Start! link: 36, page: 36, adjustment:0\n",
      "Data saved for link: 36, page: 36\n",
      "Start! link: 37, page: 37, adjustment:0\n",
      "Data saved for link: 37, page: 37\n",
      "Start! link: 38, page: 38, adjustment:0\n",
      "Data saved for link: 38, page: 38\n",
      "Start! link: 39, page: 39, adjustment:0\n",
      "Data saved for link: 39, page: 39\n",
      "Start! link: 40, page: 40, adjustment:0\n",
      "Data saved for link: 40, page: 40\n",
      "Start! link: 41, page: 41, adjustment:0\n",
      "Data saved for link: 41, page: 41\n",
      "Start! link: 42, page: 42, adjustment:0\n",
      "Data saved for link: 42, page: 42\n",
      "Start! link: 43, page: 43, adjustment:0\n",
      "Data saved for link: 43, page: 43\n",
      "Start! link: 44, page: 44, adjustment:0\n",
      "Data saved for link: 44, page: 44\n",
      "Start! link: 45, page: 45, adjustment:0\n",
      "Data saved for link: 45, page: 45\n",
      "Start! link: 46, page: 46, adjustment:0\n",
      "Start! link: 47, page: 46, adjustment:1\n",
      "Data saved for link: 47, page: 46\n",
      "Start! link: 48, page: 47, adjustment:1\n",
      "Data saved for link: 48, page: 47\n",
      "Start! link: 49, page: 48, adjustment:1\n",
      "Data saved for link: 49, page: 48\n",
      "Start! link: 50, page: 49, adjustment:1\n",
      "Data saved for link: 50, page: 49\n",
      "Start! link: 51, page: 50, adjustment:1\n",
      "Data saved for link: 51, page: 50\n",
      "Start! link: 52, page: 51, adjustment:1\n",
      "Data saved for link: 52, page: 51\n",
      "Start! link: 53, page: 52, adjustment:1\n",
      "Data saved for link: 53, page: 52\n",
      "Start! link: 54, page: 53, adjustment:1\n",
      "Data saved for link: 54, page: 53\n",
      "Start! link: 55, page: 54, adjustment:1\n",
      "Data saved for link: 55, page: 54\n",
      "Start! link: 56, page: 55, adjustment:1\n",
      "Data saved for link: 56, page: 55\n",
      "Start! link: 57, page: 56, adjustment:1\n",
      "Data saved for link: 57, page: 56\n",
      "Start! link: 58, page: 57, adjustment:1\n",
      "Data saved for link: 58, page: 57\n",
      "Start! link: 59, page: 58, adjustment:1\n",
      "Data saved for link: 59, page: 58\n",
      "Start! link: 60, page: 59, adjustment:1\n",
      "Data saved for link: 60, page: 59\n",
      "Start! link: 61, page: 60, adjustment:1\n",
      "Data saved for link: 61, page: 60\n",
      "Start! link: 62, page: 61, adjustment:1\n",
      "Data saved for link: 62, page: 61\n",
      "Start! link: 63, page: 62, adjustment:1\n",
      "Data saved for link: 63, page: 62\n",
      "Start! link: 64, page: 63, adjustment:1\n",
      "Data saved for link: 64, page: 63\n",
      "Start! link: 65, page: 64, adjustment:1\n",
      "Data saved for link: 65, page: 64\n",
      "Start! link: 66, page: 65, adjustment:1\n",
      "Data saved for link: 66, page: 65\n",
      "Start! link: 67, page: 66, adjustment:1\n",
      "Data saved for link: 67, page: 66\n",
      "Start! link: 68, page: 67, adjustment:1\n",
      "Data saved for link: 68, page: 67\n",
      "Start! link: 69, page: 68, adjustment:1\n",
      "Data saved for link: 69, page: 68\n",
      "Start! link: 70, page: 69, adjustment:1\n",
      "Data saved for link: 70, page: 69\n",
      "Start! link: 71, page: 70, adjustment:1\n",
      "Data saved for link: 71, page: 70\n",
      "Start! link: 72, page: 71, adjustment:1\n",
      "Data saved for link: 72, page: 71\n",
      "Start! link: 73, page: 72, adjustment:1\n",
      "Data saved for link: 73, page: 72\n",
      "Start! link: 74, page: 73, adjustment:1\n",
      "Data saved for link: 74, page: 73\n",
      "Start! link: 75, page: 74, adjustment:1\n",
      "Data saved for link: 75, page: 74\n",
      "Start! link: 76, page: 75, adjustment:1\n",
      "Data saved for link: 76, page: 75\n",
      "Start! link: 77, page: 76, adjustment:1\n",
      "Start! link: 78, page: 76, adjustment:2\n",
      "Data saved for link: 78, page: 76\n",
      "Start! link: 79, page: 77, adjustment:2\n",
      "Data saved for link: 79, page: 77\n",
      "Start! link: 80, page: 78, adjustment:2\n",
      "Data saved for link: 80, page: 78\n",
      "Start! link: 81, page: 79, adjustment:2\n",
      "Data saved for link: 81, page: 79\n",
      "Start! link: 82, page: 80, adjustment:2\n",
      "Data saved for link: 82, page: 80\n",
      "Start! link: 83, page: 81, adjustment:2\n",
      "Data saved for link: 83, page: 81\n",
      "Start! link: 84, page: 82, adjustment:2\n",
      "Data saved for link: 84, page: 82\n",
      "Start! link: 85, page: 83, adjustment:2\n",
      "Data saved for link: 85, page: 83\n",
      "Start! link: 86, page: 84, adjustment:2\n",
      "Data saved for link: 86, page: 84\n",
      "Start! link: 87, page: 85, adjustment:2\n",
      "Start! link: 88, page: 85, adjustment:3\n",
      "Data saved for link: 88, page: 85\n",
      "Start! link: 89, page: 86, adjustment:3\n",
      "Data saved for link: 89, page: 86\n",
      "Start! link: 90, page: 87, adjustment:3\n",
      "Data saved for link: 90, page: 87\n",
      "Start! link: 91, page: 88, adjustment:3\n",
      "Data saved for link: 91, page: 88\n",
      "Start! link: 92, page: 89, adjustment:3\n",
      "Data saved for link: 92, page: 89\n",
      "Start! link: 93, page: 90, adjustment:3\n",
      "Data saved for link: 93, page: 90\n",
      "Start! link: 94, page: 91, adjustment:3\n",
      "Data saved for link: 94, page: 91\n",
      "Start! link: 95, page: 92, adjustment:3\n",
      "Data saved for link: 95, page: 92\n",
      "Start! link: 96, page: 93, adjustment:3\n",
      "Data saved for link: 96, page: 93\n",
      "Start! link: 97, page: 94, adjustment:3\n",
      "Data saved for link: 97, page: 94\n",
      "Start! link: 98, page: 95, adjustment:3\n",
      "Data saved for link: 98, page: 95\n",
      "Start! link: 99, page: 96, adjustment:3\n",
      "Data saved for link: 99, page: 96\n",
      "Start! link: 100, page: 97, adjustment:3\n",
      "Data saved for link: 100, page: 97\n",
      "Start! link: 101, page: 98, adjustment:3\n",
      "Data saved for link: 101, page: 98\n",
      "Start! link: 102, page: 99, adjustment:3\n",
      "Data saved for link: 102, page: 99\n",
      "Start! link: 103, page: 100, adjustment:3\n",
      "Data saved for link: 103, page: 100\n",
      "Start! link: 104, page: 101, adjustment:3\n",
      "Data saved for link: 104, page: 101\n",
      "Start! link: 105, page: 102, adjustment:3\n",
      "Data saved for link: 105, page: 102\n",
      "Start! link: 106, page: 103, adjustment:3\n",
      "Start! link: 107, page: 103, adjustment:4\n",
      "Data saved for link: 107, page: 103\n",
      "Start! link: 108, page: 104, adjustment:4\n",
      "Data saved for link: 108, page: 104\n",
      "Start! link: 109, page: 105, adjustment:4\n",
      "Data saved for link: 109, page: 105\n",
      "Start! link: 110, page: 106, adjustment:4\n",
      "Data saved for link: 110, page: 106\n",
      "Start! link: 111, page: 107, adjustment:4\n",
      "Data saved for link: 111, page: 107\n",
      "Start! link: 112, page: 108, adjustment:4\n",
      "Data saved for link: 112, page: 108\n",
      "Start! link: 113, page: 109, adjustment:4\n",
      "Data saved for link: 113, page: 109\n",
      "Start! link: 114, page: 110, adjustment:4\n",
      "Data saved for link: 114, page: 110\n",
      "Start! link: 115, page: 111, adjustment:4\n",
      "Data saved for link: 115, page: 111\n",
      "Start! link: 116, page: 112, adjustment:4\n",
      "Data saved for link: 116, page: 112\n",
      "Start! link: 117, page: 113, adjustment:4\n",
      "Data saved for link: 117, page: 113\n",
      "Start! link: 118, page: 114, adjustment:4\n",
      "Data saved for link: 118, page: 114\n",
      "Start! link: 119, page: 115, adjustment:4\n",
      "Data saved for link: 119, page: 115\n",
      "Start! link: 120, page: 116, adjustment:4\n",
      "Data saved for link: 120, page: 116\n",
      "Start! link: 121, page: 117, adjustment:4\n",
      "Data saved for link: 121, page: 117\n",
      "Start! link: 122, page: 118, adjustment:4\n",
      "Data saved for link: 122, page: 118\n",
      "Start! link: 123, page: 119, adjustment:4\n",
      "Data saved for link: 123, page: 119\n",
      "Start! link: 124, page: 120, adjustment:4\n",
      "Data saved for link: 124, page: 120\n",
      "Start! link: 125, page: 121, adjustment:4\n",
      "Data saved for link: 125, page: 121\n",
      "Start! link: 126, page: 122, adjustment:4\n",
      "Data saved for link: 126, page: 122\n",
      "Start! link: 127, page: 123, adjustment:4\n",
      "Data saved for link: 127, page: 123\n",
      "Start! link: 128, page: 124, adjustment:4\n",
      "Data saved for link: 128, page: 124\n",
      "Start! link: 129, page: 125, adjustment:4\n",
      "Data saved for link: 129, page: 125\n",
      "Start! link: 130, page: 126, adjustment:4\n",
      "Data saved for link: 130, page: 126\n",
      "Start! link: 131, page: 127, adjustment:4\n",
      "Data saved for link: 131, page: 127\n",
      "Start! link: 132, page: 128, adjustment:4\n",
      "Data saved for link: 132, page: 128\n",
      "Start! link: 133, page: 129, adjustment:4\n",
      "Start! link: 134, page: 129, adjustment:5\n",
      "Start! link: 135, page: 129, adjustment:6\n",
      "Data saved for link: 135, page: 129\n",
      "Start! link: 136, page: 130, adjustment:6\n",
      "Data saved for link: 136, page: 130\n",
      "Start! link: 137, page: 131, adjustment:6\n",
      "Start! link: 138, page: 131, adjustment:7\n",
      "Start! link: 139, page: 131, adjustment:8\n",
      "Start! link: 140, page: 131, adjustment:9\n",
      "Data saved for link: 140, page: 131\n",
      "Start! link: 141, page: 132, adjustment:9\n",
      "Data saved for link: 141, page: 132\n",
      "Start! link: 142, page: 133, adjustment:9\n",
      "Data saved for link: 142, page: 133\n",
      "Start! link: 143, page: 134, adjustment:9\n",
      "Data saved for link: 143, page: 134\n",
      "Start! link: 144, page: 135, adjustment:9\n",
      "Data saved for link: 144, page: 135\n",
      "Start! link: 145, page: 136, adjustment:9\n",
      "Data saved for link: 145, page: 136\n",
      "Start! link: 146, page: 137, adjustment:9\n",
      "Data saved for link: 146, page: 137\n",
      "Start! link: 147, page: 138, adjustment:9\n",
      "Data saved for link: 147, page: 138\n",
      "Start! link: 148, page: 139, adjustment:9\n",
      "Data saved for link: 148, page: 139\n",
      "Start! link: 149, page: 140, adjustment:9\n",
      "Data saved for link: 149, page: 140\n",
      "Start! link: 150, page: 141, adjustment:9\n",
      "Start! link: 151, page: 141, adjustment:10\n",
      "Data saved for link: 151, page: 141\n",
      "Start! link: 152, page: 142, adjustment:10\n",
      "Data saved for link: 152, page: 142\n",
      "Start! link: 153, page: 143, adjustment:10\n",
      "Data saved for link: 153, page: 143\n",
      "Start! link: 154, page: 144, adjustment:10\n",
      "Data saved for link: 154, page: 144\n",
      "Start! link: 155, page: 145, adjustment:10\n",
      "Data saved for link: 155, page: 145\n",
      "Start! link: 156, page: 146, adjustment:10\n",
      "Data saved for link: 156, page: 146\n",
      "Start! link: 157, page: 147, adjustment:10\n",
      "Data saved for link: 157, page: 147\n",
      "Start! link: 158, page: 148, adjustment:10\n",
      "Data saved for link: 158, page: 148\n",
      "Start! link: 159, page: 149, adjustment:10\n",
      "Data saved for link: 159, page: 149\n",
      "Start! link: 160, page: 150, adjustment:10\n",
      "Data saved for link: 160, page: 150\n",
      "Start! link: 161, page: 151, adjustment:10\n",
      "Data saved for link: 161, page: 151\n",
      "Start! link: 162, page: 152, adjustment:10\n",
      "Data saved for link: 162, page: 152\n",
      "Start! link: 163, page: 153, adjustment:10\n",
      "Data saved for link: 163, page: 153\n",
      "Start! link: 164, page: 154, adjustment:10\n",
      "Data saved for link: 164, page: 154\n",
      "Start! link: 165, page: 155, adjustment:10\n",
      "Data saved for link: 165, page: 155\n",
      "Start! link: 166, page: 156, adjustment:10\n",
      "Data saved for link: 166, page: 156\n",
      "Start! link: 167, page: 157, adjustment:10\n",
      "Data saved for link: 167, page: 157\n",
      "\n",
      "Scraping Completed. \n",
      "total pages: 157, \n",
      "total adjustments: 88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver.maximize_window()\n",
    "link_num = 0\n",
    "adjustment = 0\n",
    "\n",
    "for article_url in article_urls:\n",
    "\n",
    "    initial_scroll = 50\n",
    "    add_scroll = 0\n",
    "    link_num += 1\n",
    "\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "\n",
    "        page_num = link_num - adjustment\n",
    "\n",
    "        print(f\"Start! link: {link_num}, page: {page_num}, adjustment:{adjustment}\")\n",
    "\n",
    "        driver.execute_script(f'window.scrollTo(0, {initial_scroll + add_scroll})')\n",
    "\n",
    "        button_session = 0\n",
    "        fails = 0\n",
    "        while button_session < 1:\n",
    "            try:\n",
    "                if fails == 3:\n",
    "                    break\n",
    "\n",
    "                claps_button = WebDriverWait(driver, 20).until(\n",
    "                                    EC.element_to_be_clickable(\n",
    "                                        driver.find_element(By.CLASS_NAME, \"pw-multi-vote-count\").find_element(By.TAG_NAME, 'button')\n",
    "                                        ))\n",
    "                claps_button.click()\n",
    "                button_session += 1\n",
    "            except:\n",
    "                fails += 1\n",
    "                add_scroll += 10\n",
    "                driver.execute_script(f'window.scrollTo(0, {initial_scroll + add_scroll})')\n",
    "\n",
    "                try:\n",
    "                    claps_icon = WebDriverWait(driver, 20).until(\n",
    "                            EC.element_to_be_clickable(\n",
    "                                driver.find_element(By.CLASS_NAME, \"pw-multi-vote-icon\").find_element(By.TAG_NAME, 'button')\n",
    "                                ))\n",
    "                    claps_icon.click()\n",
    "\n",
    "                except:\n",
    "                    time.sleep(1) \n",
    "\n",
    "                time.sleep(2)\n",
    "        try:\n",
    "            claps_text = driver.find_element(By.XPATH, \"//h2[contains(text(), 'claps from')]\").text\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                claps_text = driver.find_element(By.XPATH, \"//h2[contains(text(), 'clap from')]\").text\n",
    "            except:\n",
    "                claps_text = \"\"\n",
    "\n",
    "        pattern = r\"(\\d{1,9})\"\n",
    "\n",
    "        clap_count = int(re.findall(pattern, claps_text)[0])\n",
    "\n",
    "        unique_clap_count = int(re.findall(pattern, claps_text)[1])\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            close_claps = WebDriverWait(driver, 20).until(\n",
    "                                EC.element_to_be_clickable(driver.find_element(By.XPATH, \"//button[@aria-label='close']\")))\n",
    "\n",
    "            close_claps.click()\n",
    "        except:\n",
    "            print('close button failed')\n",
    "\n",
    "        title = driver.find_element(By.CLASS_NAME, \"pw-post-title\").text\n",
    "\n",
    "        summary = driver.find_element(By.TAG_NAME, 'section').text.strip(\" \")\n",
    "\n",
    "        image_count = int(driver.execute_script(\"return document.getElementsByTagName('section')[0].getElementsByTagName('img').length\"))\n",
    "\n",
    "        link_count = int(\n",
    "            driver.execute_script(\n",
    "                \"return document.getElementsByTagName('section')[0].getElementsByTagName('a').length\"))\n",
    "\n",
    "        blockquote_count = int(driver.execute_script(\"return document.getElementsByTagName('blockquote').length\"))\n",
    "\n",
    "        publication_date = driver.find_element(By.CLASS_NAME, \"pw-published-date\").text\n",
    "\n",
    "        reading_time_text = driver.find_element(By.CLASS_NAME, \"pw-reading-time\").text\n",
    "\n",
    "        reading_time = int(re.findall(pattern, reading_time_text)[0])\n",
    "\n",
    "        try:\n",
    "            comment_count = int(driver.find_element(By.CLASS_NAME, \"pw-responses-count\").text)\n",
    "\n",
    "        except:\n",
    "            comment_count = 0\n",
    "\n",
    "        author_url = driver.find_element(By.CLASS_NAME, \"pw-author-name\").find_element(By.XPATH, \"..\").get_attribute('href')\n",
    "\n",
    "        author_name = driver.find_element(By.CLASS_NAME, \"pw-author-name\").text.strip(\" \")\n",
    "\n",
    "        try:\n",
    "            publication_url = driver.find_element(By.XPATH, \"//div[normalize-space()='Published in']\").find_element(By.XPATH, \"../..\").find_elements(By.TAG_NAME, 'a')[1].get_attribute('href')\n",
    "            publication_name = driver.find_element(By.XPATH, \"//div[normalize-space()='Published in']\").find_element(By.XPATH, \"../..\").find_elements(By.TAG_NAME, 'a')[1].find_element(By.TAG_NAME, \"p\").text.strip(\" \")\n",
    "\n",
    "        except:\n",
    "            publication_url = None\n",
    "            publication_name = None\n",
    "\n",
    "        try:\n",
    "            codeblock_count = int(\n",
    "                driver.execute_script(\n",
    "                    \"return document.getElementsByTagName('pre').length\"))\n",
    "            br_count = 0\n",
    "            code_blocks = driver.find_elements(By.TAG_NAME,  \"pre\")\n",
    "            for code_block in code_blocks:\n",
    "                brakes_found = len(code_block.find_elements(By.TAG_NAME, 'br'))\n",
    "                br_count += brakes_found\n",
    "\n",
    "            code_count = br_count + codeblock_count\n",
    "        except:\n",
    "            codeblock_count = 0\n",
    "            code_count = 0\n",
    "\n",
    "        # all_codes_list = []\n",
    "\n",
    "        # for code_block in code_blocks:\n",
    "        #     codes_found = code_block.text.split('\\n')\n",
    "        #     all_codes_list.extend(codes_found)\n",
    "\n",
    "        article_info = {\n",
    "            \"article_url\" : article_url, # change later\n",
    "            \"best_of\": best_of,\n",
    "            \"title\": title,\n",
    "            \"summary\": summary,\n",
    "            \"image_count\": image_count,\n",
    "            \"link_count\": link_count,\n",
    "            \"blockquote_count\": blockquote_count,\n",
    "            \"publication_date\": publication_date,\n",
    "            'reading_time': reading_time,\n",
    "            \"clap_count\" : clap_count,\n",
    "            \"unique_clap_count\": unique_clap_count,\n",
    "            \"comment_count\": comment_count,\n",
    "            \"author_url\": author_url,\n",
    "            \"author_name\": author_name,\n",
    "            \"publication_url\": publication_url,\n",
    "            \"publication_name\": publication_name,\n",
    "            \"codeblock_count\": codeblock_count,\n",
    "            \"code_count\": code_count,\n",
    "            # \"all_codes_list\": all_codes_list\n",
    "        }\n",
    "\n",
    "        page_data = pd.DataFrame([article_info])\n",
    "\n",
    "        page_data.to_csv(website + '/' + start_date + '/' + best_of  + '/page' + str(page_num)+ \".csv\", index=False )\n",
    "\n",
    "        print(f\"Data saved for link: {link_num}, page: {page_num}\")\n",
    "\n",
    "        if page_num >= target: # can change to target later\n",
    "            break \n",
    "\n",
    "    except:\n",
    "        adjustment += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\"\"\n",
    "Scraping Completed. \n",
    "total pages: {page_num}, \n",
    "total adjustments: {adjustment}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start! link: 168, page: 158, adjustment:10\n",
      "Data saved for link: 168, page: 158\n",
      "Start! link: 169, page: 159, adjustment:10\n",
      "Data saved for link: 169, page: 159\n",
      "Start! link: 170, page: 160, adjustment:10\n",
      "Data saved for link: 170, page: 160\n",
      "Start! link: 171, page: 161, adjustment:10\n",
      "Data saved for link: 171, page: 161\n",
      "Start! link: 172, page: 162, adjustment:10\n",
      "Data saved for link: 172, page: 162\n",
      "Start! link: 173, page: 163, adjustment:10\n",
      "Data saved for link: 173, page: 163\n",
      "Start! link: 174, page: 164, adjustment:10\n",
      "Data saved for link: 174, page: 164\n",
      "Start! link: 175, page: 165, adjustment:10\n",
      "Data saved for link: 175, page: 165\n",
      "Start! link: 176, page: 166, adjustment:10\n",
      "Data saved for link: 176, page: 166\n",
      "Start! link: 177, page: 167, adjustment:10\n",
      "Data saved for link: 177, page: 167\n",
      "Start! link: 178, page: 168, adjustment:10\n",
      "Data saved for link: 178, page: 168\n",
      "Start! link: 179, page: 169, adjustment:10\n",
      "Data saved for link: 179, page: 169\n",
      "Start! link: 180, page: 170, adjustment:10\n",
      "Data saved for link: 180, page: 170\n",
      "Start! link: 181, page: 171, adjustment:10\n",
      "Data saved for link: 181, page: 171\n",
      "Start! link: 182, page: 172, adjustment:10\n",
      "Data saved for link: 182, page: 172\n",
      "Start! link: 183, page: 173, adjustment:10\n",
      "Data saved for link: 183, page: 173\n",
      "Start! link: 184, page: 174, adjustment:10\n",
      "Data saved for link: 184, page: 174\n",
      "Start! link: 185, page: 175, adjustment:10\n",
      "Data saved for link: 185, page: 175\n",
      "Start! link: 186, page: 176, adjustment:10\n",
      "Data saved for link: 186, page: 176\n",
      "Start! link: 187, page: 177, adjustment:10\n",
      "Data saved for link: 187, page: 177\n",
      "Start! link: 188, page: 178, adjustment:10\n",
      "Data saved for link: 188, page: 178\n",
      "Start! link: 189, page: 179, adjustment:10\n",
      "Data saved for link: 189, page: 179\n",
      "Start! link: 190, page: 180, adjustment:10\n",
      "Data saved for link: 190, page: 180\n",
      "Start! link: 191, page: 181, adjustment:10\n",
      "Data saved for link: 191, page: 181\n",
      "Start! link: 192, page: 182, adjustment:10\n",
      "Data saved for link: 192, page: 182\n",
      "Start! link: 193, page: 183, adjustment:10\n",
      "Data saved for link: 193, page: 183\n",
      "Start! link: 194, page: 184, adjustment:10\n",
      "Data saved for link: 194, page: 184\n",
      "Start! link: 195, page: 185, adjustment:10\n",
      "Data saved for link: 195, page: 185\n",
      "Start! link: 196, page: 186, adjustment:10\n",
      "Data saved for link: 196, page: 186\n",
      "Start! link: 197, page: 187, adjustment:10\n",
      "Data saved for link: 197, page: 187\n",
      "Start! link: 198, page: 188, adjustment:10\n",
      "Data saved for link: 198, page: 188\n",
      "Start! link: 199, page: 189, adjustment:10\n",
      "Data saved for link: 199, page: 189\n",
      "Start! link: 200, page: 190, adjustment:10\n",
      "Data saved for link: 200, page: 190\n",
      "Start! link: 201, page: 191, adjustment:10\n",
      "Data saved for link: 201, page: 191\n",
      "Start! link: 202, page: 192, adjustment:10\n",
      "Data saved for link: 202, page: 192\n",
      "Start! link: 203, page: 193, adjustment:10\n",
      "Data saved for link: 203, page: 193\n",
      "Start! link: 204, page: 194, adjustment:10\n",
      "Data saved for link: 204, page: 194\n",
      "Start! link: 205, page: 195, adjustment:10\n",
      "Data saved for link: 205, page: 195\n",
      "Start! link: 206, page: 196, adjustment:10\n",
      "Data saved for link: 206, page: 196\n",
      "Start! link: 207, page: 197, adjustment:10\n",
      "Data saved for link: 207, page: 197\n",
      "Start! link: 208, page: 198, adjustment:10\n",
      "Data saved for link: 208, page: 198\n",
      "Start! link: 209, page: 199, adjustment:10\n",
      "Data saved for link: 209, page: 199\n",
      "Start! link: 210, page: 200, adjustment:10\n",
      "Data saved for link: 210, page: 200\n",
      "\n",
      "Scraping Completed. \n",
      "total pages: 200, \n",
      "total adjustments: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver.maximize_window()\n",
    "link_num = 167\n",
    "adjustment = 10\n",
    "\n",
    "for article_url in article_urls:\n",
    "\n",
    "    initial_scroll = 50\n",
    "    add_scroll = 0\n",
    "    link_num += 1\n",
    "\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "\n",
    "        page_num = link_num - adjustment \n",
    "\n",
    "        print(f\"Start! link: {link_num}, page: {page_num}, adjustment:{adjustment}\")\n",
    "\n",
    "        driver.execute_script(f'window.scrollTo(0, {initial_scroll + add_scroll})')\n",
    "\n",
    "        button_session = 0\n",
    "        fails = 0\n",
    "        while button_session < 1:\n",
    "            try:\n",
    "                if fails == 3:\n",
    "                    break\n",
    "\n",
    "                claps_button = WebDriverWait(driver, 20).until(\n",
    "                                    EC.element_to_be_clickable(\n",
    "                                        driver.find_element(By.CLASS_NAME, \"pw-multi-vote-count\").find_element(By.TAG_NAME, 'button')\n",
    "                                        ))\n",
    "                claps_button.click()\n",
    "                button_session += 1\n",
    "            except:\n",
    "                fails += 1\n",
    "                add_scroll += 10\n",
    "                driver.execute_script(f'window.scrollTo(0, {initial_scroll + add_scroll})')\n",
    "\n",
    "                try:\n",
    "                    claps_icon = WebDriverWait(driver, 20).until(\n",
    "                            EC.element_to_be_clickable(\n",
    "                                driver.find_element(By.CLASS_NAME, \"pw-multi-vote-icon\").find_element(By.TAG_NAME, 'button')\n",
    "                                ))\n",
    "                    claps_icon.click()\n",
    "\n",
    "                except:\n",
    "                    time.sleep(1) \n",
    "\n",
    "                time.sleep(2)\n",
    "        try:\n",
    "            claps_text = driver.find_element(By.XPATH, \"//h2[contains(text(), 'claps from')]\").text\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                claps_text = driver.find_element(By.XPATH, \"//h2[contains(text(), 'clap from')]\").text\n",
    "            except:\n",
    "                claps_text = \"\"\n",
    "\n",
    "        pattern = r\"(\\d{1,9})\"\n",
    "\n",
    "        clap_count = int(re.findall(pattern, claps_text)[0])\n",
    "\n",
    "        unique_clap_count = int(re.findall(pattern, claps_text)[1])\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            close_claps = WebDriverWait(driver, 20).until(\n",
    "                                EC.element_to_be_clickable(driver.find_element(By.XPATH, \"//button[@aria-label='close']\")))\n",
    "\n",
    "            close_claps.click()\n",
    "        except:\n",
    "            print('close button failed')\n",
    "\n",
    "        title = driver.find_element(By.CLASS_NAME, \"pw-post-title\").text\n",
    "\n",
    "        summary = driver.find_element(By.TAG_NAME, 'section').text.strip(\" \")\n",
    "\n",
    "        image_count = int(driver.execute_script(\"return document.getElementsByTagName('section')[0].getElementsByTagName('img').length\"))\n",
    "\n",
    "        link_count = int(\n",
    "            driver.execute_script(\n",
    "                \"return document.getElementsByTagName('section')[0].getElementsByTagName('a').length\"))\n",
    "\n",
    "        blockquote_count = int(driver.execute_script(\"return document.getElementsByTagName('blockquote').length\"))\n",
    "\n",
    "        publication_date = driver.find_element(By.CLASS_NAME, \"pw-published-date\").text\n",
    "\n",
    "        reading_time_text = driver.find_element(By.CLASS_NAME, \"pw-reading-time\").text\n",
    "\n",
    "        reading_time = int(re.findall(pattern, reading_time_text)[0])\n",
    "\n",
    "        try:\n",
    "            comment_count = int(driver.find_element(By.CLASS_NAME, \"pw-responses-count\").text)\n",
    "\n",
    "        except:\n",
    "            comment_count = 0\n",
    "\n",
    "        author_url = driver.find_element(By.CLASS_NAME, \"pw-author-name\").find_element(By.XPATH, \"..\").get_attribute('href')\n",
    "\n",
    "        author_name = driver.find_element(By.CLASS_NAME, \"pw-author-name\").text.strip(\" \")\n",
    "\n",
    "        try:\n",
    "            publication_url = driver.find_element(By.XPATH, \"//div[normalize-space()='Published in']\").find_element(By.XPATH, \"../..\").find_elements(By.TAG_NAME, 'a')[1].get_attribute('href')\n",
    "            publication_name = driver.find_element(By.XPATH, \"//div[normalize-space()='Published in']\").find_element(By.XPATH, \"../..\").find_elements(By.TAG_NAME, 'a')[1].find_element(By.TAG_NAME, \"p\").text.strip(\" \")\n",
    "\n",
    "        except:\n",
    "            publication_url = None\n",
    "            publication_name = None\n",
    "\n",
    "        try:\n",
    "            codeblock_count = int(\n",
    "                driver.execute_script(\n",
    "                    \"return document.getElementsByTagName('pre').length\"))\n",
    "            br_count = 0\n",
    "            code_blocks = driver.find_elements(By.TAG_NAME,  \"pre\")\n",
    "            for code_block in code_blocks:\n",
    "                brakes_found = len(code_block.find_elements(By.TAG_NAME, 'br'))\n",
    "                br_count += brakes_found\n",
    "\n",
    "            code_count = br_count + codeblock_count\n",
    "        except:\n",
    "            codeblock_count = 0\n",
    "            code_count = 0\n",
    "\n",
    "        # all_codes_list = []\n",
    "\n",
    "        # for code_block in code_blocks:\n",
    "        #     codes_found = code_block.text.split('\\n')\n",
    "        #     all_codes_list.extend(codes_found)\n",
    "\n",
    "        article_info = {\n",
    "            \"article_url\" : article_url, # change later\n",
    "            \"best_of\": best_of,\n",
    "            \"title\": title,\n",
    "            \"summary\": summary,\n",
    "            \"image_count\": image_count,\n",
    "            \"link_count\": link_count,\n",
    "            \"blockquote_count\": blockquote_count,\n",
    "            \"publication_date\": publication_date,\n",
    "            'reading_time': reading_time,\n",
    "            \"clap_count\" : clap_count,\n",
    "            \"unique_clap_count\": unique_clap_count,\n",
    "            \"comment_count\": comment_count,\n",
    "            \"author_url\": author_url,\n",
    "            \"author_name\": author_name,\n",
    "            \"publication_url\": publication_url,\n",
    "            \"publication_name\": publication_name,\n",
    "            \"codeblock_count\": codeblock_count,\n",
    "            \"code_count\": code_count,\n",
    "            # \"all_codes_list\": all_codes_list\n",
    "        }\n",
    "\n",
    "        page_data = pd.DataFrame([article_info])\n",
    "\n",
    "        page_data.to_csv(website + '/' + start_date + '/' + best_of  + '/page' + str(page_num)+ \".csv\", index=False )\n",
    "\n",
    "        print(f\"Data saved for link: {link_num}, page: {page_num}\")\n",
    "\n",
    "        if page_num >= target: # can change to target later\n",
    "            break \n",
    "\n",
    "    except:\n",
    "        adjustment += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\"\"\n",
    "Scraping Completed. \n",
    "total pages: {page_num}, \n",
    "total adjustments: {adjustment}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading page 2....Done!\n",
      "loading page 3....Done!\n",
      "loading page 4....Done!\n",
      "loading page 5....Done!\n",
      "loading page 6....Done!\n",
      "loading page 7....Done!\n",
      "loading page 8....Done!\n",
      "loading page 9....Done!\n",
      "loading page 10....Done!\n",
      "loading page 11....Done!\n",
      "loading page 12....Done!\n",
      "loading page 13....Done!\n",
      "loading page 14....Done!\n",
      "loading page 15....Done!\n",
      "loading page 16....Done!\n",
      "loading page 17....Done!\n",
      "loading page 18....Done!\n",
      "loading page 19....Done!\n",
      "loading page 20....Done!\n",
      "loading page 21....Done!\n",
      "loading page 22....Done!\n",
      "loading page 23....Done!\n",
      "loading page 24....Done!\n",
      "loading page 25....Done!\n",
      "loading page 26....Done!\n",
      "loading page 27....Done!\n",
      "loading page 28....Done!\n",
      "loading page 29....Done!\n",
      "loading page 30....Done!\n",
      "loading page 31....Done!\n",
      "loading page 32....Done!\n",
      "loading page 33....Done!\n",
      "loading page 34....Done!\n",
      "loading page 35....Done!\n",
      "loading page 36....Done!\n",
      "loading page 37....Done!\n",
      "loading page 38....Done!\n",
      "loading page 39....Done!\n",
      "loading page 40....Done!\n",
      "loading page 41....Done!\n",
      "loading page 42....Done!\n",
      "loading page 43....Done!\n",
      "loading page 44....Done!\n",
      "loading page 45....Done!\n",
      "loading page 46....Done!\n",
      "loading page 47....Done!\n",
      "loading page 48....Done!\n",
      "loading page 49....Done!\n",
      "loading page 50....Done!\n",
      "loading page 51....Done!\n",
      "loading page 52....Done!\n",
      "loading page 53....Done!\n",
      "loading page 54....Done!\n",
      "loading page 55....Done!\n",
      "loading page 56....Done!\n",
      "loading page 57....Done!\n",
      "loading page 58....Done!\n",
      "loading page 59....Done!\n",
      "loading page 60....Done!\n",
      "loading page 61....Done!\n",
      "loading page 62....Done!\n",
      "loading page 63....Done!\n",
      "loading page 64....Done!\n",
      "loading page 65....Done!\n",
      "loading page 66....Done!\n",
      "loading page 67....Done!\n",
      "loading page 68....Done!\n",
      "loading page 69....Done!\n",
      "loading page 70....Done!\n",
      "loading page 71....Done!\n",
      "loading page 72....Done!\n",
      "loading page 73....Done!\n",
      "loading page 74....Done!\n",
      "loading page 75....Done!\n",
      "loading page 76....Done!\n",
      "loading page 77....Done!\n",
      "loading page 78....Done!\n",
      "loading page 79....Done!\n",
      "loading page 80....Done!\n",
      "loading page 81....Done!\n",
      "loading page 82....Done!\n",
      "loading page 83....Done!\n",
      "loading page 84....Done!\n",
      "loading page 85....Done!\n",
      "loading page 86....Done!\n",
      "loading page 87....Done!\n",
      "loading page 88....Done!\n",
      "loading page 89....Done!\n",
      "loading page 90....Done!\n",
      "loading page 91....Done!\n",
      "loading page 92....Done!\n",
      "loading page 93....Done!\n",
      "loading page 94....Done!\n",
      "loading page 95....Done!\n",
      "loading page 96....Done!\n",
      "loading page 97....Done!\n",
      "loading page 98....Done!\n",
      "loading page 99....Done!\n",
      "loading page 100....Done!\n",
      "loading page 101....Done!\n",
      "loading page 102....Done!\n",
      "loading page 103....Done!\n",
      "loading page 104....Done!\n",
      "loading page 105....Done!\n",
      "loading page 106....Done!\n",
      "loading page 107....Done!\n",
      "loading page 108....Done!\n",
      "loading page 109....Done!\n",
      "loading page 110....Done!\n",
      "loading page 111....Done!\n",
      "loading page 112....Done!\n",
      "loading page 113....Done!\n",
      "loading page 114....Done!\n",
      "loading page 115....Done!\n",
      "loading page 116....Done!\n",
      "loading page 117....Done!\n",
      "loading page 118....Done!\n",
      "loading page 119....Done!\n",
      "loading page 120....Done!\n",
      "loading page 121....Done!\n",
      "loading page 122....Done!\n",
      "loading page 123....Done!\n",
      "loading page 124....Done!\n",
      "loading page 125....Done!\n",
      "loading page 126....Done!\n",
      "loading page 127....Done!\n",
      "loading page 128....Done!\n",
      "loading page 129....Done!\n",
      "loading page 130....Done!\n",
      "loading page 131....Done!\n",
      "loading page 132....Done!\n",
      "loading page 133....Done!\n",
      "loading page 134....Done!\n",
      "loading page 135....Done!\n",
      "loading page 136....Done!\n",
      "loading page 137....Done!\n",
      "loading page 138....Done!\n",
      "loading page 139....Done!\n",
      "loading page 140....Done!\n",
      "loading page 141....Done!\n",
      "loading page 142....Done!\n",
      "loading page 143....Done!\n",
      "loading page 144....Done!\n",
      "loading page 145....Done!\n",
      "loading page 146....Done!\n",
      "loading page 147....Done!\n",
      "loading page 148....Done!\n",
      "loading page 149....Done!\n",
      "loading page 150....Done!\n",
      "loading page 151....Done!\n",
      "loading page 152....Done!\n",
      "loading page 153....Done!\n",
      "loading page 154....Done!\n",
      "loading page 155....Done!\n",
      "loading page 156....Done!\n",
      "loading page 157....Done!\n",
      "loading page 158....Done!\n",
      "loading page 159....Done!\n",
      "loading page 160....Done!\n",
      "loading page 161....Done!\n",
      "loading page 162....Done!\n",
      "loading page 163....Done!\n",
      "loading page 164....Done!\n",
      "loading page 165....Done!\n",
      "loading page 166....Done!\n",
      "loading page 167....Done!\n",
      "loading page 168....Done!\n",
      "loading page 169....Done!\n",
      "loading page 170....Done!\n",
      "loading page 171....Done!\n",
      "loading page 172....Done!\n",
      "loading page 173....Done!\n",
      "loading page 174....Done!\n",
      "loading page 175....Done!\n",
      "loading page 176....Done!\n",
      "loading page 177....Done!\n",
      "loading page 178....Done!\n",
      "loading page 179....Done!\n",
      "loading page 180....Done!\n",
      "loading page 181....Done!\n",
      "loading page 182....Done!\n",
      "loading page 183....Done!\n",
      "loading page 184....Done!\n",
      "loading page 185....Done!\n",
      "loading page 186....Done!\n",
      "loading page 187....Done!\n",
      "loading page 188....Done!\n",
      "loading page 189....Done!\n",
      "loading page 190....Done!\n",
      "loading page 191....Done!\n",
      "loading page 192....Done!\n",
      "loading page 193....Done!\n",
      "loading page 194....Done!\n",
      "loading page 195....Done!\n",
      "loading page 196....Done!\n",
      "loading page 197....Done!\n",
      "loading page 198....Done!\n",
      "loading page 199....Done!\n",
      "loading page 200....Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_count = len(os.listdir( \"./\" + website + \"/\" + start_date + \"/\" + best_of))\n",
    "\n",
    "result_df = pd.read_csv( \"./\" + website + \"/\" + start_date + \"/\" + best_of + \"/page1.csv\")\n",
    "\n",
    "for page in range(2,file_count+1,1):\n",
    "\n",
    "    print(f\"loading page {page}....\", end=\"\")\n",
    "\n",
    "    df_to_add = pd.read_csv( website + '/' + start_date + '/' + best_of + '/page'+ str(page)+ \".csv\")\n",
    "\n",
    "    result_df = pd.concat([result_df, df_to_add])\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "# page_data.to_csv(website + '/' + start_date + '/' + best_of  + '/page' + str(page_num)+ \".csv\", index=False )\n",
    "\n",
    "result_df.to_csv( website + '/top_' + str(file_count) + '_' + best_of + '_raw' + '.csv', index=False)\n",
    "\n",
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>best_of</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>image_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>blockquote_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>clap_count</th>\n",
       "      <th>unique_clap_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>author_url</th>\n",
       "      <th>author_name</th>\n",
       "      <th>publication_url</th>\n",
       "      <th>publication_name</th>\n",
       "      <th>codeblock_count</th>\n",
       "      <th>code_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/towards-data-science/meet-j...</td>\n",
       "      <td>This month</td>\n",
       "      <td>Meet Julia: The Future of Data Science</td>\n",
       "      <td>Meet Julia: The Future of Data Science\\nThe ne...</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>Nov 2</td>\n",
       "      <td>7</td>\n",
       "      <td>1798</td>\n",
       "      <td>518</td>\n",
       "      <td>38</td>\n",
       "      <td>https://natassha6789.medium.com/?source=---thr...</td>\n",
       "      <td>Natassha Selvaraj</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/towards-data-science/the-op...</td>\n",
       "      <td>This month</td>\n",
       "      <td>The Open-Source Spirit of Data Science</td>\n",
       "      <td>The Open-Source Spirit of Data Science\\nIt too...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>Nov 10</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.medium.com/?source=...</td>\n",
       "      <td>TDS Editors</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://medium.com/towards-data-science/how-da...</td>\n",
       "      <td>This month</td>\n",
       "      <td>How Data Scientists Level Up Their Coding Skills</td>\n",
       "      <td>How Data Scientists Level Up Their Coding Skil...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>Oct 27</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.medium.com/?source=...</td>\n",
       "      <td>TDS Editors</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medium.com/towards-data-science/data-s...</td>\n",
       "      <td>This month</td>\n",
       "      <td>Data Science Soft Skills, Revisited</td>\n",
       "      <td>Data Science Soft Skills, Revisited\\nIf you lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Nov 3</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>https://towardsdatascience.medium.com/?source=...</td>\n",
       "      <td>TDS Editors</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://medium.com/towards-data-science/discov...</td>\n",
       "      <td>This month</td>\n",
       "      <td>Discover New and Exciting Voices on TDS</td>\n",
       "      <td>Discover New and Exciting Voices on TDS\\nFew t...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Oct 20</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.medium.com/?source=...</td>\n",
       "      <td>TDS Editors</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://medium.com/geekculture/5-tools-that-ma...</td>\n",
       "      <td>This month</td>\n",
       "      <td>5 Tools That Make My Life Easier When Writing ...</td>\n",
       "      <td>5 Tools That Make My Life Easier When Writing ...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Oct 30</td>\n",
       "      <td>5</td>\n",
       "      <td>259</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>https://medium.com/@frank-andrade?source=---th...</td>\n",
       "      <td>Frank Andrade</td>\n",
       "      <td>https://medium.com/geekculture?source=post_pag...</td>\n",
       "      <td>Geek Culture</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://medium.com/towards-data-science/life-l...</td>\n",
       "      <td>This month</td>\n",
       "      <td>Life Lessons I Learned from Working as a Data ...</td>\n",
       "      <td>Teluk Hijau, Banyuwangi (Image by Author)\\nLif...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>Oct 25</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>https://tanuwidjajaolivia.medium.com/?source=-...</td>\n",
       "      <td>Olivia Tanuwidjaja</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://medium.com/@joulee/the-data-informed-m...</td>\n",
       "      <td>This month</td>\n",
       "      <td>Manifesto for the Data-Informed</td>\n",
       "      <td>Manifesto for the Data-Informed\\nWhy is the pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Nov 4</td>\n",
       "      <td>5</td>\n",
       "      <td>957</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>https://joulee.medium.com/?source=---three_col...</td>\n",
       "      <td>Julie Zhuo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://medium.com/codex/dont-use-iterrows-for...</td>\n",
       "      <td>This month</td>\n",
       "      <td>Follow This Approach to run 31x FASTER loops i...</td>\n",
       "      <td>Follow This Approach to run 31x FASTER loops i...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Oct 26</td>\n",
       "      <td>5</td>\n",
       "      <td>303</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>https://medium.com/@anmol3015?source=---three_...</td>\n",
       "      <td>Anmol Tomar</td>\n",
       "      <td>https://medium.com/codex?source=post_page-----...</td>\n",
       "      <td>CodeX</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>https://medium.com/techtofreedom/9-fabulous-py...</td>\n",
       "      <td>This month</td>\n",
       "      <td>9 Fabulous Python Tricks That Make Your Code M...</td>\n",
       "      <td>PYTHON\\n9 Fabulous Python Tricks That Make You...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Nov 14</td>\n",
       "      <td>5</td>\n",
       "      <td>299</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.com/@yangzhou1993?source=---thr...</td>\n",
       "      <td>Yang Zhou</td>\n",
       "      <td>https://medium.com/techtofreedom?source=post_p...</td>\n",
       "      <td>TechToFreedom</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           article_url     best_of  \\\n",
       "0    https://medium.com/towards-data-science/meet-j...  This month   \n",
       "1    https://medium.com/towards-data-science/the-op...  This month   \n",
       "2    https://medium.com/towards-data-science/how-da...  This month   \n",
       "3    https://medium.com/towards-data-science/data-s...  This month   \n",
       "4    https://medium.com/towards-data-science/discov...  This month   \n",
       "..                                                 ...         ...   \n",
       "195  https://medium.com/geekculture/5-tools-that-ma...  This month   \n",
       "196  https://medium.com/towards-data-science/life-l...  This month   \n",
       "197  https://medium.com/@joulee/the-data-informed-m...  This month   \n",
       "198  https://medium.com/codex/dont-use-iterrows-for...  This month   \n",
       "199  https://medium.com/techtofreedom/9-fabulous-py...  This month   \n",
       "\n",
       "                                                 title  \\\n",
       "0               Meet Julia: The Future of Data Science   \n",
       "1               The Open-Source Spirit of Data Science   \n",
       "2     How Data Scientists Level Up Their Coding Skills   \n",
       "3                  Data Science Soft Skills, Revisited   \n",
       "4              Discover New and Exciting Voices on TDS   \n",
       "..                                                 ...   \n",
       "195  5 Tools That Make My Life Easier When Writing ...   \n",
       "196  Life Lessons I Learned from Working as a Data ...   \n",
       "197                    Manifesto for the Data-Informed   \n",
       "198  Follow This Approach to run 31x FASTER loops i...   \n",
       "199  9 Fabulous Python Tricks That Make Your Code M...   \n",
       "\n",
       "                                               summary  image_count  \\\n",
       "0    Meet Julia: The Future of Data Science\\nThe ne...            7   \n",
       "1    The Open-Source Spirit of Data Science\\nIt too...            1   \n",
       "2    How Data Scientists Level Up Their Coding Skil...            1   \n",
       "3    Data Science Soft Skills, Revisited\\nIf you lo...            1   \n",
       "4    Discover New and Exciting Voices on TDS\\nFew t...            1   \n",
       "..                                                 ...          ...   \n",
       "195  5 Tools That Make My Life Easier When Writing ...            8   \n",
       "196  Teluk Hijau, Banyuwangi (Image by Author)\\nLif...            6   \n",
       "197  Manifesto for the Data-Informed\\nWhy is the pr...            1   \n",
       "198  Follow This Approach to run 31x FASTER loops i...            4   \n",
       "199  PYTHON\\n9 Fabulous Python Tricks That Make You...            2   \n",
       "\n",
       "     link_count  blockquote_count publication_date  reading_time  clap_count  \\\n",
       "0            30                 0            Nov 2             7        1798   \n",
       "1            22                 0           Nov 10             3          35   \n",
       "2            21                 0           Oct 27             3          95   \n",
       "3            25                 0            Nov 3             3         134   \n",
       "4            24                 0           Oct 20             3          20   \n",
       "..          ...               ...              ...           ...         ...   \n",
       "195           9                 0           Oct 30             5         259   \n",
       "196          13                 5           Oct 25             7          84   \n",
       "197           2                 0            Nov 4             5         957   \n",
       "198           5                 1           Oct 26             5         303   \n",
       "199           4                 1           Nov 14             5         299   \n",
       "\n",
       "     unique_clap_count  comment_count  \\\n",
       "0                  518             38   \n",
       "1                   12              3   \n",
       "2                   36              2   \n",
       "3                   18              0   \n",
       "4                   10              1   \n",
       "..                 ...            ...   \n",
       "195                 79              4   \n",
       "196                 39              0   \n",
       "197                126             10   \n",
       "198                 82              9   \n",
       "199                 60              5   \n",
       "\n",
       "                                            author_url         author_name  \\\n",
       "0    https://natassha6789.medium.com/?source=---thr...   Natassha Selvaraj   \n",
       "1    https://towardsdatascience.medium.com/?source=...         TDS Editors   \n",
       "2    https://towardsdatascience.medium.com/?source=...         TDS Editors   \n",
       "3    https://towardsdatascience.medium.com/?source=...         TDS Editors   \n",
       "4    https://towardsdatascience.medium.com/?source=...         TDS Editors   \n",
       "..                                                 ...                 ...   \n",
       "195  https://medium.com/@frank-andrade?source=---th...       Frank Andrade   \n",
       "196  https://tanuwidjajaolivia.medium.com/?source=-...  Olivia Tanuwidjaja   \n",
       "197  https://joulee.medium.com/?source=---three_col...          Julie Zhuo   \n",
       "198  https://medium.com/@anmol3015?source=---three_...         Anmol Tomar   \n",
       "199  https://medium.com/@yangzhou1993?source=---thr...           Yang Zhou   \n",
       "\n",
       "                                       publication_url      publication_name  \\\n",
       "0    https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "1    https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "2    https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "3    https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "4    https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "..                                                 ...                   ...   \n",
       "195  https://medium.com/geekculture?source=post_pag...          Geek Culture   \n",
       "196  https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "197                                                NaN                   NaN   \n",
       "198  https://medium.com/codex?source=post_page-----...                 CodeX   \n",
       "199  https://medium.com/techtofreedom?source=post_p...         TechToFreedom   \n",
       "\n",
       "     codeblock_count  code_count  \n",
       "0                  1           2  \n",
       "1                  0           0  \n",
       "2                  0           0  \n",
       "3                  0           0  \n",
       "4                  0           0  \n",
       "..               ...         ...  \n",
       "195                2           5  \n",
       "196                0           0  \n",
       "197                0           0  \n",
       "198                6          55  \n",
       "199                8          26  \n",
       "\n",
       "[200 rows x 18 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top200_df = pd.read_csv( website + '/top_' + str(file_count) + '_' + best_of + '_raw' + '.csv')\n",
    "top200_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
