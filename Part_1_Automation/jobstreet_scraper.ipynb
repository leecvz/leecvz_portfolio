{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.expected_conditions import visibility_of_element_located\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "start_date = now.strftime('%m%d%Y')\n",
    "# make data/playlist/ directory\n",
    "os.makedirs('jobstreet/' + start_date , exist_ok=True)\n",
    "chrome_path = r'C:/Users/user/scraper_sel/chromedriver.exe' # This is the location of the chronedriver on my local machine\n",
    "\n",
    "service = Service(executable_path=chrome_path)  \n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "\n",
    "url = r'http://www.jobstreet.com.ph/'\n",
    "\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(8)\n",
    "\n",
    "\n",
    "keyword = 'Data Science'\n",
    "location = 'Philippines'\n",
    "\n",
    "keyword_field = driver.find_element(By.ID, 'searchKeywordsField')\n",
    "keyword_field.clear()\n",
    "keyword_field.send_keys(keyword)\n",
    "\n",
    "location_field = driver.find_element(By.ID, 'locationAutoSuggest')\n",
    "location_field.clear()\n",
    "location_field.send_keys(location)\n",
    "\n",
    "find = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "find.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvesting page: 1....Saved!\n",
      "Harvesting page: 2....Saved!\n"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "\n",
    "while True\n",
    "\n",
    "    job_post_index = 0\n",
    "\n",
    "    try:\n",
    "\n",
    "        print(f'Harvesting page: {page}....', end=\"\")\n",
    "\n",
    "        articles = driver.find_elements(By.TAG_NAME, \"article\")\n",
    "\n",
    "        time.sleep(1 + 2*random.random())\n",
    "        to_frame = []\n",
    "        items = 0\n",
    "\n",
    "        for article in articles:\n",
    "        \n",
    "            time.sleep(1 + 2*random.random())\n",
    "\n",
    "            job_button = WebDriverWait(driver, 20).until(\n",
    "                    EC.element_to_be_clickable(article))\n",
    "\n",
    "            try:\n",
    "\n",
    "                driver.execute_script('arguments[0].click();', job_button)\n",
    "                items += 1\n",
    "\n",
    "                try:\n",
    "                    summary = driver.find_element(By.XPATH, \"//div[@data-automation='jobDescription']\").text.strip()\n",
    "\n",
    "                except:\n",
    "                    summary = None\n",
    "                \n",
    "            except:\n",
    "                print(f'click {job_post_index} failed')\n",
    "\n",
    "            location = article.find_elements(By.TAG_NAME, 'span')[3].text.strip()\n",
    "            title = article.find_elements(By.TAG_NAME, 'span')[0].text.strip()\n",
    "            company = article.find_elements(By.TAG_NAME, \"span\")[1].text.strip()\n",
    "\n",
    "            test_salary = article.find_elements(By.TAG_NAME, \"span\")[5].text.strip()\n",
    "\n",
    "            if re.search('php', test_salary.lower()):\n",
    "                salary = test_salary\n",
    "            else:\n",
    "                salary = None\n",
    "\n",
    "            # try to get url,\n",
    "            job_url = article.find_elements(By.TAG_NAME, 'a')[0].get_attribute('href')\n",
    "         \n",
    "            job_info = {\n",
    "                'location': location,\n",
    "                'title': title,\n",
    "                'company': company,\n",
    "                'salary': salary,\n",
    "                'summary': summary,\n",
    "                'job_url': job_url}\n",
    "\n",
    "            to_frame.append(job_info)\n",
    "            job_post_index += 1\n",
    "        \n",
    "        framed_df = pd.DataFrame(to_frame)\n",
    "        framed_df.to_csv('jobstreet/' + start_date + '/page' + str(page) + '.csv', index=False)\n",
    "        print('Saved!')\n",
    "\n",
    "        start_url = driver.current_url\n",
    "\n",
    "        next_page = WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.XPATH,\n",
    "                \"//span[normalize-space()='Next']\"\n",
    "                )))\n",
    "\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "\n",
    "        next_url = driver.current_url\n",
    "\n",
    "        if start_url == next_url:\n",
    "            print(f\"Scraper stopped moving, scaping ended at page {page - 1}\")\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    except:\n",
    "        print(f\"Exception hit, scaping ended at page {page - 1}\")\n",
    "        break\n",
    "\n",
    "print('/nScrape End')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines all scraped files into 1 raw file\n",
    "\n",
    "file_count = len(os.listdir(\"./jobstreet/\" + start_date ))\n",
    "\n",
    "result_df = pd.read_csv('jobstreet/' + start_date +'/page1.csv')\n",
    "\n",
    "for page in range(2,file_count,1):\n",
    "\n",
    "    print(f\"loading page {page}....\", end=\"\")\n",
    "\n",
    "    df_to_add = pd.read_csv('jobstreet/' + start_date + '/page'+ str(page) + '.csv')\n",
    "\n",
    "    result_df = pd.concat([result_df, df_to_add])\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "result_df.to_csv('jobstreet/raw_' + start_date + '/page' + str(page) + '.csv', index=False)\n",
    "\n",
    "result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
